# Quantum Multi-Backend Agent — Part 4: Benchmarking & Parallel Execution

_Previous: [agent3.md](agent3.md) — Execution Systems_
_Next: [agent5.md](agent5.md) — Comparative Analysis & Visualization_

---

## Overview

This file defines how Crush should run benchmarks across multiple backends, execute parameter sweeps, and run multiple backends simultaneously on the same specifications.

---

## Benchmark Workflow

When a user says "benchmark all backends on GHZ 10 qubits," Crush should:

1. Parse the request into a structured benchmark config.
2. Check which backends are installed and ready.
3. Generate a benchmark plan listing each (backend, config) pair.
4. Execute each run (sequentially or in parallel as requested).
5. Collect all results into the standardized JSON schema.
6. Produce a summary comparison.

---

## Benchmark Configuration Schema

```json
{
  "benchmark_id": "bench_20260206_001",
  "name": "GHZ Scalability",
  "description": "Compare all backends on GHZ circuits from 4 to 20 qubits",
  "backends": ["lret-phase7", "cirq", "qiskit-aer", "qsim", "pennylane", "stim"],
  "parameters": {
    "n_qubits": [4, 6, 8, 10, 12, 14, 16, 18, 20],
    "circuit_type": ["ghz"],
    "shots": [1000],
    "noise_model": [null],
    "noise_strength": [0.0]
  },
  "execution": {
    "parallel": true,
    "max_parallel": 4,
    "timeout_per_run": 300,
    "retries": 1,
    "warmup_runs": 1
  },
  "output_dir": "results/bench_20260206_001"
}
```

This produces `len(n_qubits) x len(backends)` = 54 total runs.

---

## Natural Language to Benchmark Mapping

| User Says | Benchmark Plan |
|-----------|----------------|
| "benchmark all backends on GHZ 10 qubits" | All installed backends, GHZ, n_qubits=10 |
| "compare Cirq and Qiskit on random circuits 4 to 16 qubits" | cirq + qiskit-aer, random, n_qubits=[4,6,8,10,12,14,16] |
| "run scalability test for LRET" | lret-phase7, GHZ, n_qubits=[4,6,8,...,24] |
| "benchmark with noise 0.01 and 0.05" | All backends, noise_strength=[0.01, 0.05] |
| "compare GPU vs CPU on Qiskit Aer" | qiskit-aer x2 (device=CPU, device=GPU) |
| "run everything at 20 qubits" | All installed backends, n_qubits=20 |

---

## Benchmark Runner Script

Crush should generate and execute this script for complex benchmarks:

```python
#!/usr/bin/env python3
"""
Multi-backend benchmark runner.
Generated by Crush for: {benchmark_name}
"""
import json, os, time, sys, traceback
from datetime import datetime
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed

# ─── Configuration ───────────────────────────────────────────────
BENCHMARK_ID = "bench_{timestamp}"
OUTPUT_DIR = Path("results") / BENCHMARK_ID
BACKENDS = ["cirq", "qiskit-aer", "qsim", "pennylane", "stim"]
N_QUBITS_RANGE = [4, 6, 8, 10, 12, 14, 16]
CIRCUIT_TYPE = "ghz"
SHOTS = 1000
MAX_PARALLEL = 4
TIMEOUT = 300  # seconds per run
WARMUP = 1     # warmup runs (discarded)

# ─── Backend Runners ─────────────────────────────────────────────

def run_single(backend, n_qubits, circuit_type, shots, run_index):
    """Run a single backend simulation and return standardized result."""
    try:
        if backend == "cirq":
            from run_cirq import run_cirq
            return run_cirq(n_qubits=n_qubits, circuit_type=circuit_type, shots=shots)
        elif backend == "qiskit-aer":
            from run_qiskit_aer import run_qiskit_aer
            return run_qiskit_aer(n_qubits=n_qubits, circuit_type=circuit_type, shots=shots)
        elif backend == "qsim":
            from run_qsim import run_qsim
            return run_qsim(n_qubits=n_qubits, circuit_type=circuit_type)
        elif backend == "pennylane":
            from run_pennylane import run_pennylane
            return run_pennylane(n_qubits=n_qubits, circuit_type=circuit_type, shots=shots)
        elif backend == "stim":
            from run_stim import run_stim
            return run_stim(n_qubits=n_qubits, circuit_type=circuit_type, shots=shots)
        elif backend.startswith("lret"):
            from run_lret import run_lret
            return run_lret(n_qubits=n_qubits, circuit_type=circuit_type, variant=backend)
        else:
            return {"status": "error", "error": f"Unknown backend: {backend}"}
    except Exception as e:
        return {
            "status": "error",
            "backend": backend,
            "config": {"n_qubits": n_qubits},
            "error": str(e),
            "traceback": traceback.format_exc(),
        }

# ─── Main Benchmark Loop ────────────────────────────────────────

def run_benchmark():
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    all_results = []
    total_runs = len(BACKENDS) * len(N_QUBITS_RANGE)
    completed = 0
    started_at = datetime.now().isoformat()

    print(f"Benchmark: {BENCHMARK_ID}")
    print(f"Backends:  {', '.join(BACKENDS)}")
    print(f"Qubits:    {N_QUBITS_RANGE}")
    print(f"Total runs: {total_runs} (+ {WARMUP * total_runs} warmup)")
    print(f"Max parallel: {MAX_PARALLEL}")
    print("=" * 60)

    # Warmup phase
    if WARMUP > 0:
        print("Running warmup...")
        for backend in BACKENDS:
            for _ in range(WARMUP):
                try:
                    run_single(backend, min(N_QUBITS_RANGE), CIRCUIT_TYPE, 10, 0)
                except Exception:
                    pass

    # Main runs
    with ProcessPoolExecutor(max_workers=MAX_PARALLEL) as executor:
        futures = {}
        for backend in BACKENDS:
            for nq in N_QUBITS_RANGE:
                key = f"{backend}_{nq}q"
                future = executor.submit(
                    run_single, backend, nq, CIRCUIT_TYPE, SHOTS, 0
                )
                futures[future] = key

        for future in as_completed(futures, timeout=TIMEOUT * total_runs):
            key = futures[future]
            completed += 1
            try:
                result = future.result(timeout=TIMEOUT)
                all_results.append(result)
                status = result.get("status", "unknown")
                t = result.get("metrics", {}).get("execution_time_seconds", "?")
                print(f"  [{completed}/{total_runs}] {key}: {status} ({t}s)")
            except Exception as e:
                all_results.append({"status": "error", "key": key, "error": str(e)})
                print(f"  [{completed}/{total_runs}] {key}: ERROR ({e})")

            # Save individual result
            result_file = OUTPUT_DIR / f"{key}.json"
            with open(result_file, 'w') as f:
                json.dump(result, f, indent=2, default=str)

    # Save summary
    summary = {
        "benchmark_id": BENCHMARK_ID,
        "started_at": started_at,
        "completed_at": datetime.now().isoformat(),
        "total_runs": total_runs,
        "successful": sum(1 for r in all_results if r.get("status") == "success"),
        "failed": sum(1 for r in all_results if r.get("status") != "success"),
        "results": all_results,
    }
    with open(OUTPUT_DIR / "summary.json", 'w') as f:
        json.dump(summary, f, indent=2, default=str)

    print(f"\nComplete: {summary['successful']}/{total_runs} successful")
    print(f"Results saved to: {OUTPUT_DIR}/")
    return summary

if __name__ == "__main__":
    run_benchmark()
```

---

## Parallel Execution

### Running Multiple Backends Simultaneously

When a user says "run Cirq, Qiskit, and Stim at the same time on 10 qubits GHZ":

```python
#!/usr/bin/env python3
"""Run multiple backends in parallel on the same specifications."""
from concurrent.futures import ProcessPoolExecutor, as_completed
import json, time

def run_parallel(backends, config, max_workers=None):
    """Execute multiple backends in parallel with identical config."""
    if max_workers is None:
        max_workers = len(backends)

    results = {}
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = {}
        for backend in backends:
            future = executor.submit(run_single, backend, **config)
            futures[future] = backend

        for future in as_completed(futures, timeout=600):
            backend = futures[future]
            try:
                results[backend] = future.result()
                print(f"  {backend}: done ({results[backend]['metrics']['execution_time_seconds']:.3f}s)")
            except Exception as e:
                results[backend] = {"status": "error", "error": str(e)}
                print(f"  {backend}: FAILED ({e})")

    return results

# Usage:
config = {"n_qubits": 10, "circuit_type": "ghz", "shots": 1000}
results = run_parallel(["cirq", "qiskit-aer", "stim"], config)
```

### Parallel Execution Safety Rules

1. **Resource Limits**: Default to `min(len(backends), os.cpu_count())` workers.
2. **Memory Check**: Before parallel runs, estimate total memory. Each backend with N qubits uses ~`2^N * 16` bytes for the state vector. For N > 25 qubits, warn the user.
3. **GPU Contention**: Only one GPU backend at a time unless user has multiple GPUs.
4. **Result Isolation**: Each backend runs in a separate process to avoid import conflicts.
5. **Timeout**: Individual runs have a per-run timeout (default 300s).

---

## Parameter Sweeps

### Qubit Scalability Sweep

```python
def qubit_sweep(backends, qubit_range, circuit_type="ghz", shots=1000):
    """Sweep qubit count for all backends."""
    all_results = []
    for nq in qubit_range:
        print(f"\n--- {nq} qubits ---")
        for backend in backends:
            result = run_single(backend, nq, circuit_type, shots, 0)
            all_results.append(result)
            t = result.get("metrics", {}).get("execution_time_seconds", "?")
            print(f"  {backend}: {t}s")
    return all_results

# Example:
qubit_sweep(["cirq", "qiskit-aer", "stim"], range(4, 22, 2))
```

### Noise Sweep

```python
def noise_sweep(backends, noise_levels, n_qubits=10, circuit_type="ghz"):
    """Sweep noise strength for all backends."""
    all_results = []
    for noise in noise_levels:
        print(f"\n--- noise={noise} ---")
        for backend in backends:
            result = run_single(backend, n_qubits, circuit_type, 1000, 0)
            # Override noise config for this run
            result["config"]["noise_strength"] = noise
            all_results.append(result)
    return all_results

# Example:
noise_sweep(["cirq", "qiskit-aer"], [0.0, 0.001, 0.01, 0.05, 0.1])
```

### Circuit Depth Sweep

```python
def depth_sweep(backends, depths, n_qubits=10, circuit_type="random"):
    """Sweep circuit depth for all backends."""
    all_results = []
    for d in depths:
        print(f"\n--- depth={d} ---")
        for backend in backends:
            result = run_single(backend, n_qubits, circuit_type, 1000, 0)
            result["config"]["depth"] = d
            all_results.append(result)
    return all_results

# Example:
depth_sweep(["cirq", "qiskit-aer", "qsim"], [10, 20, 50, 100, 200])
```

---

## Benchmark Natural Language Commands for Crush

Users can issue these commands directly in the Crush TUI:

| Command | What Crush Does |
|---------|----------------|
| "benchmark all backends on GHZ 10 qubits" | Run every installed backend, GHZ circuit, 10 qubits, 1000 shots |
| "scalability test from 4 to 24 qubits" | Qubit sweep on all backends, step=2 |
| "noise benchmark at 0.01, 0.05, 0.1" | Noise sweep on noise-supporting backends |
| "benchmark LRET vs Cirq on random depth 50" | Two backends, random circuit, depth=50 |
| "run parallel benchmark all backends 16 qubits" | All backends simultaneously, 16 qubits |
| "benchmark Qiskit GPU vs CPU 20 qubits" | Qiskit Aer with method=statevector_gpu and CPU |
| "sweep circuit types: ghz, qft, random" | All circuit types across all backends |
| "rerun failed benchmarks from last run" | Re-execute only the failed entries from the most recent benchmark |

---

## Benchmark Result Storage Structure

```
results/
├── bench_20260206_001/
│   ├── summary.json               # Overall benchmark summary
│   ├── cirq_4q.json               # Individual run results
│   ├── cirq_6q.json
│   ├── cirq_8q.json
│   ├── qiskit-aer_4q.json
│   ├── stim_4q.json
│   └── comparison.json            # Cross-backend comparison (see agent5.md)
├── bench_20260206_002/
│   └── ...
└── latest -> bench_20260206_002   # Symlink to most recent
```

---

## Backend Availability Check

Before running benchmarks, Crush should verify which backends are available:

```python
#!/usr/bin/env python3
"""Check which backends are available for benchmarking."""
import importlib, shutil

BACKEND_CHECKS = {
    "cirq": lambda: importlib.import_module("cirq"),
    "qiskit-aer": lambda: importlib.import_module("qiskit_aer"),
    "qsim": lambda: importlib.import_module("qsimcirq"),
    "pennylane": lambda: importlib.import_module("pennylane"),
    "stim": lambda: importlib.import_module("stim"),
    "cuquantum": lambda: importlib.import_module("cuquantum"),
    "lret-phase7": lambda: shutil.which("quantum_sim") or
        __import__("pathlib").Path("backends/lret-phase7/build/quantum_sim").exists(),
    "lret-cirq-scalability": lambda: __import__("pathlib").Path(
        "backends/lret-cirq-scalability/build/quantum_sim").exists(),
    "lret-pennylane": lambda: __import__("pathlib").Path(
        "backends/lret-pennylane/build/quantum_sim").exists(),
    "quest": lambda: __import__("pathlib").Path(
        "backends/quest/build/libQuEST.so").exists(),
}

available = []
unavailable = []
for name, check in BACKEND_CHECKS.items():
    try:
        result = check()
        if result is not None and not result:
            unavailable.append(name)
        else:
            available.append(name)
    except Exception:
        unavailable.append(name)

print(f"Available:   {', '.join(available)}")
print(f"Unavailable: {', '.join(unavailable)}")
```

---

_Continue to [agent5.md](agent5.md) for Comparative Analysis & Visualization — metrics, charts, and report generation._
